{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae25d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTTransformer\n",
    "import torch\n",
    "from minbpe.gpt4 import GPT4Tokenizer\n",
    "from minbpe.basic import BasicTokenizer\n",
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(model_file='../output/tokenizer/temp_tokenizer.model')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "context_size = 512\n",
    "embedding_dimension = 256\n",
    "no_of_attention_heads = 8\n",
    "key_query_reduced_dimensionality = 8\n",
    "no_of_blocks = 16\n",
    "batch_size = 16\n",
    "vocab_size = len(tokenizer.vocab)+len(tokenizer.special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd7389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.664456\n"
     ]
    }
   ],
   "source": [
    "model = GPTTransformer(context_size, no_of_blocks, embedding_dimension, key_query_reduced_dimensionality, no_of_attention_heads)\n",
    "model = model.to(device)\n",
    "print(sum(i.numel() for i in model.parameters())/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3aff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ''\n",
    "with open('../output/data.txt', 'r', encoding='utf-8') as fp:\n",
    "    data = '\\n'.join(fp.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf9b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_seq = tokenizer.encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bc02ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69874"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = torch.tensor(token_seq, dtype=torch.long)\n",
    "splitter = int(0.95*len(input_data))\n",
    "\n",
    "training_data, test_data = input_data[:splitter], input_data[splitter:]\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9815902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSet(Dataset):\n",
    "    def __init__(self, data: torch.Tensor, block_size: int):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.data[index: index + self.block_size]\n",
    "        y = self.data[index + 1: index + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "def dataloaders(\n",
    "    train_data: torch.Tensor,\n",
    "    test_data: torch.Tensor,\n",
    "    block_size: int,\n",
    "    batch_size: int,\n",
    "    device: torch.device) -> Tuple[DataLoader, DataLoader]:\n",
    "    training_dataset = TextDataSet(train_data.to(device), block_size)\n",
    "    test_dataset = TextDataSet(test_data.to(device), block_size)\n",
    "\n",
    "    train_loader = DataLoader(training_dataset, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a9e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = dataloaders(\n",
    "    train_data = training_data,\n",
    "    test_data = test_data,\n",
    "    block_size = context_size,\n",
    "    batch_size = batch_size,\n",
    "    device = device\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce79337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def get_model_loss(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    evaluation_iterations: int\n",
    ") -> Dict[str, float]:\n",
    "    losses = dict()\n",
    "    model.eval()\n",
    "    for test_type, loader in [('train', train_loader), ('test', test_loader)]:\n",
    "        loss = torch.zeros(evaluation_iterations)\n",
    "        index = 0\n",
    "        for x, y in loader:\n",
    "            if index >= evaluation_iterations:\n",
    "                break\n",
    "            with torch.no_grad():\n",
    "                _, loss_value = model(x, y)\n",
    "            loss[index] = loss_value.item()\n",
    "            index += 1\n",
    "        losses[test_type] = loss.mean().item()\n",
    "    model.train()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba7eded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTTransformer,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    filename: str = \"checkpoint.pth\"\n",
    "):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'loss': loss,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e60a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "evaluation_iterations = 100\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "train_loader, test_loader = dataloaders(\n",
    "    train_data=training_data,\n",
    "    test_data=test_data,\n",
    "    block_size=context_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "training_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f24bad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, step: 0, train_loss: 0.07777127623558044, test_loss: 0.07772952318191528\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m%\u001b[39m evaluation_iterations \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m batch_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m----> 4\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluation_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_iterations\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m     output, loss \u001b[38;5;241m=\u001b[39m model(x, y)\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36mget_model_loss\u001b[1;34m(model, train_loader, test_loader, evaluation_iterations)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     18\u001b[0m             _, loss_value \u001b[38;5;241m=\u001b[39m model(x, y)\n\u001b[1;32m---> 19\u001b[0m         loss[index] \u001b[38;5;241m=\u001b[39m \u001b[43mloss_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     losses[test_type] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        if batch_index % evaluation_iterations == 0 or batch_index == len(train_loader) - 1:\n",
    "            losses = get_model_loss(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                test_loader=test_loader,\n",
    "                evaluation_iterations=evaluation_iterations\n",
    "            )\n",
    "\n",
    "            print(f'Epoch: {epoch}, step: {batch_index}, train_loss: {losses[\"train\"]}, test_loss: {losses[\"test\"]}')\n",
    "        \n",
    "        output, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        loss=loss.item(),\n",
    "        filename=f'checkpoint_{epoch}.pth'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
