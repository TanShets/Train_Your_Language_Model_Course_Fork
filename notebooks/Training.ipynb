{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ae25d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTTransformer\n",
    "import torch\n",
    "from minbpe.gpt4 import GPT4Tokenizer\n",
    "from minbpe.basic import BasicTokenizer\n",
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(model_file='../output/tokenizer/temp_tokenizer.model')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "context_size = 512\n",
    "embedding_dimension = 512\n",
    "no_of_attention_heads = 8\n",
    "key_query_reduced_dimensionality = 16\n",
    "no_of_blocks = 8\n",
    "batch_size = 8\n",
    "vocab_size = len(tokenizer.vocab)+len(tokenizer.special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bd7389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.285896\n"
     ]
    }
   ],
   "source": [
    "model = GPTTransformer(context_size, no_of_blocks, embedding_dimension, key_query_reduced_dimensionality, no_of_attention_heads)\n",
    "model = model.to(device)\n",
    "print(sum(i.numel() for i in model.parameters())/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c3aff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ''\n",
    "with open('../output/data.txt', 'r', encoding='utf-8') as fp:\n",
    "    data = '\\n'.join(fp.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acf9b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_seq = tokenizer.encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bc02ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73285"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = torch.tensor(token_seq, dtype=torch.long)\n",
    "splitter = int(0.95*len(input_data))\n",
    "\n",
    "training_data, test_data = input_data[:splitter], input_data[splitter:]\n",
    "training_data.to(device=device)\n",
    "test_data.to(device=device)\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9815902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSet(Dataset):\n",
    "    def __init__(self, data: torch.Tensor, block_size: int):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.data[index: index + self.block_size]\n",
    "        y = self.data[index + 1: index + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "def dataloaders(\n",
    "    train_data: torch.Tensor,\n",
    "    test_data: torch.Tensor,\n",
    "    block_size: int,\n",
    "    batch_size: int,\n",
    "    device: torch.device) -> Tuple[DataLoader, DataLoader]:\n",
    "    training_dataset = TextDataSet(train_data.to(device), block_size)\n",
    "    test_dataset = TextDataSet(test_data.to(device), block_size)\n",
    "\n",
    "    train_loader = DataLoader(training_dataset, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4a9e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = dataloaders(\n",
    "    train_data = training_data,\n",
    "    test_data = test_data,\n",
    "    block_size = context_size,\n",
    "    batch_size = batch_size,\n",
    "    device = device\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8efa8362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9097 419\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ce79337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def get_model_loss(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    evaluation_iterations: int\n",
    ") -> Dict[str, float]:\n",
    "    losses = dict()\n",
    "    model.eval()\n",
    "    for test_type, loader in [('train', train_loader), ('test', test_loader)]:\n",
    "        loss = torch.zeros(evaluation_iterations)\n",
    "        index = 0\n",
    "        for x, y in loader:\n",
    "            if index >= evaluation_iterations:\n",
    "                break\n",
    "            with torch.no_grad():\n",
    "                _, loss_value = model(x, y)\n",
    "            loss[index] = loss_value.item()\n",
    "            index += 1\n",
    "        losses[test_type] = loss.mean().item()\n",
    "    model.train()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba7eded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTTransformer,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    filename: str = \"../output/checkpoints/checkpoint.pth\"\n",
    "):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'loss': loss,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e60a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "evaluation_iterations = 100\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "train_loader, test_loader = dataloaders(\n",
    "    train_data=training_data,\n",
    "    test_data=test_data,\n",
    "    block_size=context_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "training_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c05a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case .pth files have been created for checkpointing\n",
    "checkpoint = torch.load('../output/checkpoints/model2/checkpoint_5.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f24bad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, step: 0, train_loss: 7.786613941192627, test_loss: 7.793278694152832\n",
      "Epoch: 0, step: 100, train_loss: 6.812404155731201, test_loss: 6.839859485626221\n",
      "Epoch: 0, step: 200, train_loss: 6.106412887573242, test_loss: 6.447627067565918\n",
      "Epoch: 0, step: 300, train_loss: 5.456924915313721, test_loss: 6.079659461975098\n",
      "Epoch: 0, step: 400, train_loss: 5.032464504241943, test_loss: 5.881594657897949\n",
      "Epoch: 0, step: 500, train_loss: 4.708528995513916, test_loss: 5.7481465339660645\n",
      "Epoch: 0, step: 600, train_loss: 4.520367622375488, test_loss: 5.6883368492126465\n",
      "Epoch: 0, step: 700, train_loss: 4.332666397094727, test_loss: 5.627553939819336\n",
      "Epoch: 0, step: 800, train_loss: 4.146803855895996, test_loss: 5.638917922973633\n",
      "Epoch: 0, step: 900, train_loss: 3.991748094558716, test_loss: 5.631406307220459\n",
      "Epoch: 0, step: 1000, train_loss: 3.830354690551758, test_loss: 5.610384941101074\n",
      "Epoch: 0, step: 1100, train_loss: 3.695180892944336, test_loss: 5.6441969871521\n",
      "Epoch: 0, step: 1200, train_loss: 3.5328316688537598, test_loss: 5.723421096801758\n",
      "Epoch: 0, step: 1300, train_loss: 3.3631131649017334, test_loss: 5.730210781097412\n",
      "Epoch: 0, step: 1400, train_loss: 3.228501081466675, test_loss: 5.7924418449401855\n",
      "Epoch: 0, step: 1500, train_loss: 3.082796573638916, test_loss: 5.87772274017334\n",
      "Epoch: 0, step: 1600, train_loss: 2.929619550704956, test_loss: 5.952092170715332\n",
      "Epoch: 0, step: 1700, train_loss: 2.78584361076355, test_loss: 6.006585597991943\n",
      "Epoch: 0, step: 1800, train_loss: 2.577463388442993, test_loss: 6.09043025970459\n",
      "Epoch: 0, step: 1900, train_loss: 2.396515130996704, test_loss: 6.196627140045166\n",
      "Epoch: 0, step: 2000, train_loss: 2.2595672607421875, test_loss: 6.314568996429443\n",
      "Epoch: 0, step: 2100, train_loss: 2.1196603775024414, test_loss: 6.400927543640137\n",
      "Epoch: 0, step: 2200, train_loss: 1.988612174987793, test_loss: 6.502397537231445\n",
      "Epoch: 0, step: 2300, train_loss: 1.8243415355682373, test_loss: 6.584250450134277\n",
      "Epoch: 0, step: 2400, train_loss: 1.6959524154663086, test_loss: 6.671712875366211\n",
      "Epoch: 0, step: 2500, train_loss: 1.5782872438430786, test_loss: 6.792520523071289\n",
      "Epoch: 0, step: 2600, train_loss: 1.4779170751571655, test_loss: 6.943676948547363\n",
      "Epoch: 0, step: 2700, train_loss: 1.3854502439498901, test_loss: 6.985694408416748\n",
      "Epoch: 0, step: 2800, train_loss: 1.3072006702423096, test_loss: 7.109246253967285\n",
      "Epoch: 0, step: 2900, train_loss: 1.2150124311447144, test_loss: 7.202377796173096\n",
      "Epoch: 0, step: 3000, train_loss: 1.1561338901519775, test_loss: 7.243109226226807\n",
      "Epoch: 0, step: 3100, train_loss: 1.097842812538147, test_loss: 7.3769378662109375\n",
      "Epoch: 0, step: 3200, train_loss: 1.0636627674102783, test_loss: 7.462677001953125\n",
      "Epoch: 0, step: 3300, train_loss: 1.0163637399673462, test_loss: 7.463621616363525\n",
      "Epoch: 0, step: 3400, train_loss: 0.9808261394500732, test_loss: 7.5546417236328125\n",
      "Epoch: 0, step: 3500, train_loss: 0.963630199432373, test_loss: 7.6450629234313965\n",
      "Epoch: 0, step: 3600, train_loss: 0.9317057132720947, test_loss: 7.711569309234619\n",
      "Epoch: 0, step: 3700, train_loss: 0.9075026512145996, test_loss: 7.746037006378174\n",
      "Epoch: 0, step: 3800, train_loss: 0.8997409343719482, test_loss: 7.837262153625488\n",
      "Epoch: 0, step: 3900, train_loss: 0.8753129839897156, test_loss: 7.826322078704834\n",
      "Epoch: 0, step: 4000, train_loss: 0.8691902160644531, test_loss: 7.896353721618652\n",
      "Epoch: 0, step: 4100, train_loss: 0.8564433455467224, test_loss: 7.899703502655029\n",
      "Epoch: 0, step: 4200, train_loss: 0.8479328155517578, test_loss: 7.930568218231201\n",
      "Epoch: 0, step: 4300, train_loss: 0.8360716104507446, test_loss: 8.023235321044922\n",
      "Epoch: 0, step: 4400, train_loss: 0.8278390765190125, test_loss: 7.955016613006592\n",
      "Epoch: 0, step: 4500, train_loss: 0.8241854310035706, test_loss: 8.016986846923828\n",
      "Epoch: 0, step: 4600, train_loss: 0.8148531913757324, test_loss: 8.032698631286621\n",
      "Epoch: 0, step: 4700, train_loss: 0.8143168091773987, test_loss: 8.037489891052246\n",
      "Epoch: 0, step: 4800, train_loss: 0.8032995462417603, test_loss: 8.067758560180664\n",
      "Epoch: 0, step: 4900, train_loss: 0.7948567271232605, test_loss: 8.105831146240234\n",
      "Epoch: 0, step: 5000, train_loss: 0.8071648478507996, test_loss: 8.099299430847168\n",
      "Epoch: 0, step: 5100, train_loss: 0.7878139019012451, test_loss: 8.122605323791504\n",
      "Epoch: 0, step: 5200, train_loss: 0.7822554111480713, test_loss: 8.1171236038208\n",
      "Epoch: 0, step: 5300, train_loss: 0.7858220934867859, test_loss: 8.16168212890625\n",
      "Epoch: 0, step: 5400, train_loss: 0.771358847618103, test_loss: 8.186220169067383\n",
      "Epoch: 0, step: 5500, train_loss: 0.7759803533554077, test_loss: 8.177066802978516\n",
      "Epoch: 0, step: 5600, train_loss: 0.7721701860427856, test_loss: 8.224239349365234\n",
      "Epoch: 0, step: 5700, train_loss: 0.7631820440292358, test_loss: 8.194096565246582\n",
      "Epoch: 0, step: 5800, train_loss: 0.758962869644165, test_loss: 8.255309104919434\n",
      "Epoch: 0, step: 5900, train_loss: 0.7516056895256042, test_loss: 8.215169906616211\n",
      "Epoch: 0, step: 6000, train_loss: 0.7468698024749756, test_loss: 8.226222038269043\n",
      "Epoch: 0, step: 6100, train_loss: 0.7434828877449036, test_loss: 8.243025779724121\n",
      "Epoch: 0, step: 6200, train_loss: 0.74114590883255, test_loss: 8.216769218444824\n",
      "Epoch: 0, step: 6300, train_loss: 0.7337155342102051, test_loss: 8.297353744506836\n",
      "Epoch: 0, step: 6400, train_loss: 0.7299953699111938, test_loss: 8.277314186096191\n",
      "Epoch: 0, step: 6500, train_loss: 0.7245602607727051, test_loss: 8.260027885437012\n",
      "Epoch: 0, step: 6600, train_loss: 0.7196072340011597, test_loss: 8.246882438659668\n",
      "Epoch: 0, step: 6700, train_loss: 0.7103404998779297, test_loss: 8.31573486328125\n",
      "Epoch: 0, step: 6800, train_loss: 0.6981220841407776, test_loss: 8.331958770751953\n",
      "Epoch: 0, step: 6900, train_loss: 0.6983516812324524, test_loss: 8.329571723937988\n",
      "Epoch: 0, step: 7000, train_loss: 0.6900268793106079, test_loss: 8.388528823852539\n",
      "Epoch: 0, step: 7100, train_loss: 0.6798270344734192, test_loss: 8.380899429321289\n",
      "Epoch: 0, step: 7200, train_loss: 0.668159008026123, test_loss: 8.373344421386719\n",
      "Epoch: 0, step: 7300, train_loss: 0.6580131649971008, test_loss: 8.409036636352539\n",
      "Epoch: 0, step: 7400, train_loss: 0.6563129425048828, test_loss: 8.427044868469238\n",
      "Epoch: 0, step: 7500, train_loss: 0.641106128692627, test_loss: 8.492779731750488\n",
      "Epoch: 0, step: 7600, train_loss: 0.6345015168190002, test_loss: 8.448695182800293\n",
      "Epoch: 0, step: 7700, train_loss: 0.6328929662704468, test_loss: 8.47988510131836\n",
      "Epoch: 0, step: 7800, train_loss: 0.6216226816177368, test_loss: 8.505375862121582\n",
      "Epoch: 0, step: 7900, train_loss: 0.6123207211494446, test_loss: 8.557950019836426\n",
      "Epoch: 0, step: 8000, train_loss: 0.6007905602455139, test_loss: 8.576798439025879\n",
      "Epoch: 0, step: 8100, train_loss: 0.6064199805259705, test_loss: 8.627477645874023\n",
      "Epoch: 0, step: 8200, train_loss: 0.5893089771270752, test_loss: 8.636404037475586\n",
      "Epoch: 0, step: 8300, train_loss: 0.5859178304672241, test_loss: 8.652351379394531\n",
      "Epoch: 0, step: 8400, train_loss: 0.5805579423904419, test_loss: 8.665445327758789\n",
      "Epoch: 0, step: 8500, train_loss: 0.56918865442276, test_loss: 8.672979354858398\n",
      "Epoch: 0, step: 8600, train_loss: 0.5622208118438721, test_loss: 8.751999855041504\n",
      "Epoch: 0, step: 8700, train_loss: 0.5625000596046448, test_loss: 8.788910865783691\n",
      "Epoch: 0, step: 8800, train_loss: 0.5572091937065125, test_loss: 8.807903289794922\n",
      "Epoch: 0, step: 8900, train_loss: 0.5447180271148682, test_loss: 8.761280059814453\n",
      "Epoch: 0, step: 9000, train_loss: 0.5389904975891113, test_loss: 8.796814918518066\n",
      "Epoch: 0, step: 9096, train_loss: 0.5268495082855225, test_loss: 8.866035461425781\n",
      "Epoch: 1, step: 0, train_loss: 0.5326182842254639, test_loss: 8.873188018798828\n",
      "Epoch: 1, step: 100, train_loss: 0.5241727232933044, test_loss: 8.849157333374023\n",
      "Epoch: 1, step: 200, train_loss: 0.5267168283462524, test_loss: 8.915543556213379\n",
      "Epoch: 1, step: 300, train_loss: 0.5119078755378723, test_loss: 8.935973167419434\n",
      "Epoch: 1, step: 400, train_loss: 0.5089773535728455, test_loss: 8.897500991821289\n",
      "Epoch: 1, step: 500, train_loss: 0.5035276412963867, test_loss: 8.90115737915039\n",
      "Epoch: 1, step: 600, train_loss: 0.4971970021724701, test_loss: 8.985405921936035\n",
      "Epoch: 1, step: 700, train_loss: 0.49819546937942505, test_loss: 9.000068664550781\n",
      "Epoch: 1, step: 800, train_loss: 0.49687567353248596, test_loss: 9.016966819763184\n",
      "Epoch: 1, step: 900, train_loss: 0.48588183522224426, test_loss: 9.038579940795898\n",
      "Epoch: 1, step: 1000, train_loss: 0.4843182861804962, test_loss: 9.159764289855957\n",
      "Epoch: 1, step: 1100, train_loss: 0.4722363352775574, test_loss: 9.049835205078125\n",
      "Epoch: 1, step: 1200, train_loss: 0.4710026681423187, test_loss: 9.085526466369629\n",
      "Epoch: 1, step: 1300, train_loss: 0.4685521423816681, test_loss: 9.104369163513184\n",
      "Epoch: 1, step: 1400, train_loss: 0.4717651307582855, test_loss: 9.089086532592773\n",
      "Epoch: 1, step: 1500, train_loss: 0.4655033349990845, test_loss: 9.164880752563477\n",
      "Epoch: 1, step: 1600, train_loss: 0.45586323738098145, test_loss: 9.096243858337402\n",
      "Epoch: 1, step: 1700, train_loss: 0.45079532265663147, test_loss: 9.177013397216797\n",
      "Epoch: 1, step: 1800, train_loss: 0.45174452662467957, test_loss: 9.230059623718262\n",
      "Epoch: 1, step: 1900, train_loss: 0.44776153564453125, test_loss: 9.219112396240234\n",
      "Epoch: 1, step: 2000, train_loss: 0.44624465703964233, test_loss: 9.229938507080078\n",
      "Epoch: 1, step: 2100, train_loss: 0.44399333000183105, test_loss: 9.243136405944824\n",
      "Epoch: 1, step: 2200, train_loss: 0.44419440627098083, test_loss: 9.283863067626953\n",
      "Epoch: 1, step: 2300, train_loss: 0.4354856610298157, test_loss: 9.281148910522461\n",
      "Epoch: 1, step: 2400, train_loss: 0.42707645893096924, test_loss: 9.262696266174316\n",
      "Epoch: 1, step: 2500, train_loss: 0.4201282858848572, test_loss: 9.328597068786621\n",
      "Epoch: 1, step: 2600, train_loss: 0.42414969205856323, test_loss: 9.375801086425781\n",
      "Epoch: 1, step: 2700, train_loss: 0.41579726338386536, test_loss: 9.327962875366211\n",
      "Epoch: 1, step: 2800, train_loss: 0.4105806052684784, test_loss: 9.339061737060547\n",
      "Epoch: 1, step: 2900, train_loss: 0.4144498109817505, test_loss: 9.395119667053223\n",
      "Epoch: 1, step: 3000, train_loss: 0.40856993198394775, test_loss: 9.379555702209473\n",
      "Epoch: 1, step: 3100, train_loss: 0.4052368104457855, test_loss: 9.3692626953125\n",
      "Epoch: 1, step: 3200, train_loss: 0.40705037117004395, test_loss: 9.472640037536621\n",
      "Epoch: 1, step: 3300, train_loss: 0.4036042094230652, test_loss: 9.449783325195312\n",
      "Epoch: 1, step: 3400, train_loss: 0.3941071331501007, test_loss: 9.462013244628906\n",
      "Epoch: 1, step: 3500, train_loss: 0.3936346173286438, test_loss: 9.395092964172363\n",
      "Epoch: 1, step: 3600, train_loss: 0.389283686876297, test_loss: 9.36452579498291\n",
      "Epoch: 1, step: 3700, train_loss: 0.3855801522731781, test_loss: 9.474220275878906\n",
      "Epoch: 1, step: 3800, train_loss: 0.3897232711315155, test_loss: 9.52902603149414\n",
      "Epoch: 1, step: 3900, train_loss: 0.384158730506897, test_loss: 9.528576850891113\n",
      "Epoch: 1, step: 4000, train_loss: 0.3864026963710785, test_loss: 9.544930458068848\n",
      "Epoch: 1, step: 4100, train_loss: 0.38065704703330994, test_loss: 9.5713529586792\n",
      "Epoch: 1, step: 4200, train_loss: 0.3765714168548584, test_loss: 9.513689994812012\n",
      "Epoch: 1, step: 4300, train_loss: 0.3832367956638336, test_loss: 9.554378509521484\n",
      "Epoch: 1, step: 4400, train_loss: 0.37594878673553467, test_loss: 9.592991828918457\n",
      "Epoch: 1, step: 4500, train_loss: 0.376067578792572, test_loss: 9.592205047607422\n",
      "Epoch: 1, step: 4600, train_loss: 0.36473068594932556, test_loss: 9.645157814025879\n",
      "Epoch: 1, step: 4700, train_loss: 0.37920695543289185, test_loss: 9.564525604248047\n",
      "Epoch: 1, step: 4800, train_loss: 0.3547675311565399, test_loss: 9.554316520690918\n",
      "Epoch: 1, step: 4900, train_loss: 0.36347660422325134, test_loss: 9.531108856201172\n",
      "Epoch: 1, step: 5000, train_loss: 0.36136913299560547, test_loss: 9.550914764404297\n",
      "Epoch: 1, step: 5100, train_loss: 0.3569376468658447, test_loss: 9.639913558959961\n",
      "Epoch: 1, step: 5200, train_loss: 0.35356131196022034, test_loss: 9.582515716552734\n",
      "Epoch: 1, step: 5300, train_loss: 0.35960784554481506, test_loss: 9.622642517089844\n",
      "Epoch: 1, step: 5400, train_loss: 0.35486456751823425, test_loss: 9.607198715209961\n",
      "Epoch: 1, step: 5500, train_loss: 0.3539320230484009, test_loss: 9.619778633117676\n",
      "Epoch: 1, step: 5600, train_loss: 0.3453764021396637, test_loss: 9.655477523803711\n",
      "Epoch: 1, step: 5700, train_loss: 0.3453291356563568, test_loss: 9.631146430969238\n",
      "Epoch: 1, step: 5800, train_loss: 0.34872448444366455, test_loss: 9.622489929199219\n",
      "Epoch: 1, step: 5900, train_loss: 0.3407851457595825, test_loss: 9.73219108581543\n",
      "Epoch: 1, step: 6000, train_loss: 0.3381299674510956, test_loss: 9.658681869506836\n",
      "Epoch: 1, step: 6100, train_loss: 0.33389654755592346, test_loss: 9.794783592224121\n",
      "Epoch: 1, step: 6200, train_loss: 0.33615410327911377, test_loss: 9.659921646118164\n",
      "Epoch: 1, step: 6300, train_loss: 0.3300313651561737, test_loss: 9.682320594787598\n",
      "Epoch: 1, step: 6400, train_loss: 0.32696112990379333, test_loss: 9.649959564208984\n",
      "Epoch: 1, step: 6500, train_loss: 0.3273850977420807, test_loss: 9.758230209350586\n",
      "Epoch: 1, step: 6600, train_loss: 0.3214226961135864, test_loss: 9.791342735290527\n",
      "Epoch: 1, step: 6700, train_loss: 0.3247219920158386, test_loss: 9.792078971862793\n",
      "Epoch: 1, step: 6800, train_loss: 0.32006698846817017, test_loss: 9.793936729431152\n",
      "Epoch: 1, step: 6900, train_loss: 0.3294390141963959, test_loss: 9.763548851013184\n",
      "Epoch: 1, step: 7000, train_loss: 0.31860673427581787, test_loss: 9.765205383300781\n",
      "Epoch: 1, step: 7100, train_loss: 0.3240061104297638, test_loss: 9.853170394897461\n",
      "Epoch: 1, step: 7200, train_loss: 0.31818896532058716, test_loss: 9.815532684326172\n",
      "Epoch: 1, step: 7300, train_loss: 0.3192145526409149, test_loss: 9.832230567932129\n",
      "Epoch: 1, step: 7400, train_loss: 0.3168952167034149, test_loss: 9.742542266845703\n",
      "Epoch: 1, step: 7500, train_loss: 0.3186355233192444, test_loss: 9.820536613464355\n",
      "Epoch: 1, step: 7600, train_loss: 0.3078263998031616, test_loss: 9.859780311584473\n",
      "Epoch: 1, step: 7700, train_loss: 0.3068873882293701, test_loss: 9.914334297180176\n",
      "Epoch: 1, step: 7800, train_loss: 0.3104653060436249, test_loss: 9.852394104003906\n",
      "Epoch: 1, step: 7900, train_loss: 0.302817702293396, test_loss: 9.840259552001953\n",
      "Epoch: 1, step: 8000, train_loss: 0.30934083461761475, test_loss: 9.935853958129883\n",
      "Epoch: 1, step: 8100, train_loss: 0.3029393255710602, test_loss: 9.927781105041504\n",
      "Epoch: 1, step: 8200, train_loss: 0.31287094950675964, test_loss: 9.850515365600586\n",
      "Epoch: 1, step: 8300, train_loss: 0.30978792905807495, test_loss: 9.917272567749023\n",
      "Epoch: 1, step: 8400, train_loss: 0.3068345785140991, test_loss: 9.884578704833984\n",
      "Epoch: 1, step: 8500, train_loss: 0.30138224363327026, test_loss: 9.909089088439941\n",
      "Epoch: 1, step: 8600, train_loss: 0.30349886417388916, test_loss: 9.85287094116211\n",
      "Epoch: 1, step: 8700, train_loss: 0.29690247774124146, test_loss: 9.940291404724121\n",
      "Epoch: 1, step: 8800, train_loss: 0.2994053363800049, test_loss: 9.882187843322754\n",
      "Epoch: 1, step: 8900, train_loss: 0.29499438405036926, test_loss: 9.864177703857422\n",
      "Epoch: 1, step: 9000, train_loss: 0.30714842677116394, test_loss: 9.906781196594238\n",
      "Epoch: 1, step: 9096, train_loss: 0.28921374678611755, test_loss: 9.873244285583496\n",
      "Epoch: 2, step: 0, train_loss: 0.2886323034763336, test_loss: 9.875082015991211\n",
      "Epoch: 2, step: 100, train_loss: 0.2850101590156555, test_loss: 9.879059791564941\n",
      "Epoch: 2, step: 200, train_loss: 0.29603859782218933, test_loss: 9.94616985321045\n",
      "Epoch: 2, step: 300, train_loss: 0.30059298872947693, test_loss: 10.036517143249512\n",
      "Epoch: 2, step: 400, train_loss: 0.28321608901023865, test_loss: 9.940603256225586\n",
      "Epoch: 2, step: 500, train_loss: 0.28939929604530334, test_loss: 10.009764671325684\n",
      "Epoch: 2, step: 600, train_loss: 0.2845451831817627, test_loss: 9.968079566955566\n",
      "Epoch: 2, step: 700, train_loss: 0.28016379475593567, test_loss: 10.04648208618164\n",
      "Epoch: 2, step: 800, train_loss: 0.2825164198875427, test_loss: 10.049835205078125\n",
      "Epoch: 2, step: 900, train_loss: 0.29184719920158386, test_loss: 9.954599380493164\n",
      "Epoch: 2, step: 1000, train_loss: 0.2836852967739105, test_loss: 10.04024887084961\n",
      "Epoch: 2, step: 1100, train_loss: 0.2820327877998352, test_loss: 9.985885620117188\n",
      "Epoch: 2, step: 1200, train_loss: 0.28456243872642517, test_loss: 10.026473999023438\n",
      "Epoch: 2, step: 1300, train_loss: 0.2791104316711426, test_loss: 9.965372085571289\n",
      "Epoch: 2, step: 1400, train_loss: 0.28126952052116394, test_loss: 9.897687911987305\n",
      "Epoch: 2, step: 1500, train_loss: 0.27918052673339844, test_loss: 10.032361030578613\n",
      "Epoch: 2, step: 1600, train_loss: 0.27693039178848267, test_loss: 10.050679206848145\n",
      "Epoch: 2, step: 1700, train_loss: 0.2756796181201935, test_loss: 10.066298484802246\n",
      "Epoch: 2, step: 1800, train_loss: 0.2842513918876648, test_loss: 10.060007095336914\n",
      "Epoch: 2, step: 1900, train_loss: 0.28482046723365784, test_loss: 9.987335205078125\n",
      "Epoch: 2, step: 2000, train_loss: 0.2794889211654663, test_loss: 10.06679916381836\n",
      "Epoch: 2, step: 2100, train_loss: 0.2751598060131073, test_loss: 10.025476455688477\n",
      "Epoch: 2, step: 2200, train_loss: 0.27679184079170227, test_loss: 10.037842750549316\n",
      "Epoch: 2, step: 2300, train_loss: 0.2685936689376831, test_loss: 10.04901123046875\n",
      "Epoch: 2, step: 2400, train_loss: 0.2688453495502472, test_loss: 10.065831184387207\n",
      "Epoch: 2, step: 2500, train_loss: 0.26564738154411316, test_loss: 10.040609359741211\n",
      "Epoch: 2, step: 2600, train_loss: 0.270057737827301, test_loss: 10.045909881591797\n",
      "Epoch: 2, step: 2700, train_loss: 0.2730776071548462, test_loss: 10.096323013305664\n",
      "Epoch: 2, step: 2800, train_loss: 0.27214816212654114, test_loss: 10.093101501464844\n",
      "Epoch: 2, step: 2900, train_loss: 0.2645724415779114, test_loss: 10.094517707824707\n",
      "Epoch: 2, step: 3000, train_loss: 0.266063392162323, test_loss: 10.1244535446167\n",
      "Epoch: 2, step: 3100, train_loss: 0.25949856638908386, test_loss: 10.135117530822754\n",
      "Epoch: 2, step: 3200, train_loss: 0.2580031454563141, test_loss: 10.133628845214844\n",
      "Epoch: 2, step: 3300, train_loss: 0.25640445947647095, test_loss: 10.120115280151367\n",
      "Epoch: 2, step: 3400, train_loss: 0.2745685279369354, test_loss: 10.013409614562988\n",
      "Epoch: 2, step: 3500, train_loss: 0.2630886137485504, test_loss: 10.084181785583496\n",
      "Epoch: 2, step: 3600, train_loss: 0.2605513036251068, test_loss: 10.107982635498047\n",
      "Epoch: 2, step: 3700, train_loss: 0.26204800605773926, test_loss: 9.9854736328125\n",
      "Epoch: 2, step: 3800, train_loss: 0.26317551732063293, test_loss: 10.080933570861816\n",
      "Epoch: 2, step: 3900, train_loss: 0.262798547744751, test_loss: 10.035901069641113\n",
      "Epoch: 2, step: 4000, train_loss: 0.25233012437820435, test_loss: 10.044447898864746\n",
      "Epoch: 2, step: 4100, train_loss: 0.2578297555446625, test_loss: 10.111371040344238\n",
      "Epoch: 2, step: 4200, train_loss: 0.2616632580757141, test_loss: 10.077906608581543\n",
      "Epoch: 2, step: 4300, train_loss: 0.25662127137184143, test_loss: 10.14138126373291\n",
      "Epoch: 2, step: 4400, train_loss: 0.25334301590919495, test_loss: 10.039935111999512\n",
      "Epoch: 2, step: 4500, train_loss: 0.24776902794837952, test_loss: 10.112138748168945\n",
      "Epoch: 2, step: 4600, train_loss: 0.24967508018016815, test_loss: 10.102431297302246\n",
      "Epoch: 2, step: 4700, train_loss: 0.24782945215702057, test_loss: 10.129839897155762\n",
      "Epoch: 2, step: 4800, train_loss: 0.2570500075817108, test_loss: 10.104323387145996\n",
      "Epoch: 2, step: 4900, train_loss: 0.24542337656021118, test_loss: 10.120820045471191\n",
      "Epoch: 2, step: 5000, train_loss: 0.24343879520893097, test_loss: 10.1121187210083\n",
      "Epoch: 2, step: 5100, train_loss: 0.25448325276374817, test_loss: 10.103199005126953\n",
      "Epoch: 2, step: 5200, train_loss: 0.251979261636734, test_loss: 10.151568412780762\n",
      "Epoch: 2, step: 5300, train_loss: 0.24099846184253693, test_loss: 10.145800590515137\n",
      "Epoch: 2, step: 5400, train_loss: 0.24287991225719452, test_loss: 10.097280502319336\n",
      "Epoch: 2, step: 5500, train_loss: 0.24120403826236725, test_loss: 10.174156188964844\n",
      "Epoch: 2, step: 5600, train_loss: 0.25057879090309143, test_loss: 10.117715835571289\n",
      "Epoch: 2, step: 5700, train_loss: 0.2482483834028244, test_loss: 10.1933012008667\n",
      "Epoch: 2, step: 5800, train_loss: 0.2508848309516907, test_loss: 10.058807373046875\n",
      "Epoch: 2, step: 5900, train_loss: 0.24186290800571442, test_loss: 10.141613960266113\n",
      "Epoch: 2, step: 6000, train_loss: 0.23639698326587677, test_loss: 10.08450984954834\n",
      "Epoch: 2, step: 6100, train_loss: 0.24473927915096283, test_loss: 10.156330108642578\n",
      "Epoch: 2, step: 6200, train_loss: 0.23930227756500244, test_loss: 10.1201810836792\n",
      "Epoch: 2, step: 6300, train_loss: 0.23771978914737701, test_loss: 10.18940258026123\n",
      "Epoch: 2, step: 6400, train_loss: 0.24755775928497314, test_loss: 10.146293640136719\n",
      "Epoch: 2, step: 6500, train_loss: 0.23971013724803925, test_loss: 10.091741561889648\n",
      "Epoch: 2, step: 6600, train_loss: 0.2388249635696411, test_loss: 10.1300048828125\n",
      "Epoch: 2, step: 6700, train_loss: 0.2393263280391693, test_loss: 10.103865623474121\n",
      "Epoch: 2, step: 6800, train_loss: 0.23355713486671448, test_loss: 10.152597427368164\n",
      "Epoch: 2, step: 6900, train_loss: 0.2336268424987793, test_loss: 10.158157348632812\n",
      "Epoch: 2, step: 7000, train_loss: 0.23275578022003174, test_loss: 10.181203842163086\n",
      "Epoch: 2, step: 7100, train_loss: 0.2323756366968155, test_loss: 10.185870170593262\n",
      "Epoch: 2, step: 7200, train_loss: 0.23561571538448334, test_loss: 10.188488006591797\n",
      "Epoch: 2, step: 7300, train_loss: 0.24531233310699463, test_loss: 10.163302421569824\n",
      "Epoch: 2, step: 7400, train_loss: 0.23272569477558136, test_loss: 10.129375457763672\n",
      "Epoch: 2, step: 7500, train_loss: 0.23745031654834747, test_loss: 10.12918472290039\n",
      "Epoch: 2, step: 7600, train_loss: 0.2492578625679016, test_loss: 10.135135650634766\n",
      "Epoch: 2, step: 7700, train_loss: 0.2380560338497162, test_loss: 10.056756973266602\n",
      "Epoch: 2, step: 7800, train_loss: 0.23129558563232422, test_loss: 10.1876859664917\n",
      "Epoch: 2, step: 7900, train_loss: 0.245137020945549, test_loss: 10.096551895141602\n",
      "Epoch: 2, step: 8000, train_loss: 0.22519052028656006, test_loss: 10.179366111755371\n",
      "Epoch: 2, step: 8100, train_loss: 0.23781433701515198, test_loss: 10.127327919006348\n",
      "Epoch: 2, step: 8200, train_loss: 0.2192946821451187, test_loss: 10.069621086120605\n",
      "Epoch: 2, step: 8300, train_loss: 0.22863726317882538, test_loss: 10.14440631866455\n",
      "Epoch: 2, step: 8400, train_loss: 0.22910146415233612, test_loss: 10.152961730957031\n",
      "Epoch: 2, step: 8500, train_loss: 0.2265293002128601, test_loss: 10.149083137512207\n",
      "Epoch: 2, step: 8600, train_loss: 0.2325945347547531, test_loss: 10.092035293579102\n",
      "Epoch: 2, step: 8700, train_loss: 0.22058214247226715, test_loss: 10.189888000488281\n",
      "Epoch: 2, step: 8800, train_loss: 0.22755643725395203, test_loss: 10.093844413757324\n",
      "Epoch: 2, step: 8900, train_loss: 0.2240096926689148, test_loss: 10.182884216308594\n",
      "Epoch: 2, step: 9000, train_loss: 0.22471821308135986, test_loss: 10.20711898803711\n",
      "Epoch: 2, step: 9096, train_loss: 0.22878819704055786, test_loss: 10.198188781738281\n",
      "Epoch: 3, step: 0, train_loss: 0.2289000004529953, test_loss: 10.202255249023438\n",
      "Epoch: 3, step: 100, train_loss: 0.22534292936325073, test_loss: 10.246416091918945\n",
      "Epoch: 3, step: 200, train_loss: 0.22115443646907806, test_loss: 10.133583068847656\n",
      "Epoch: 3, step: 300, train_loss: 0.22208350896835327, test_loss: 10.173274040222168\n",
      "Epoch: 3, step: 400, train_loss: 0.21966388821601868, test_loss: 10.224935531616211\n",
      "Epoch: 3, step: 500, train_loss: 0.2366739809513092, test_loss: 10.24972152709961\n",
      "Epoch: 3, step: 600, train_loss: 0.22718091309070587, test_loss: 10.172484397888184\n",
      "Epoch: 3, step: 700, train_loss: 0.23127338290214539, test_loss: 10.155123710632324\n",
      "Epoch: 3, step: 800, train_loss: 0.2286495417356491, test_loss: 10.189364433288574\n",
      "Epoch: 3, step: 900, train_loss: 0.23258963227272034, test_loss: 10.201279640197754\n",
      "Epoch: 3, step: 1000, train_loss: 0.2351200431585312, test_loss: 10.171738624572754\n",
      "Epoch: 3, step: 1100, train_loss: 0.21990598738193512, test_loss: 10.207390785217285\n",
      "Epoch: 3, step: 1200, train_loss: 0.2174077183008194, test_loss: 10.188570976257324\n",
      "Epoch: 3, step: 1300, train_loss: 0.22852236032485962, test_loss: 10.22488784790039\n",
      "Epoch: 3, step: 1400, train_loss: 0.21852712333202362, test_loss: 10.281411170959473\n",
      "Epoch: 3, step: 1500, train_loss: 0.23138299584388733, test_loss: 10.176654815673828\n",
      "Epoch: 3, step: 1600, train_loss: 0.22298093140125275, test_loss: 10.220478057861328\n",
      "Epoch: 3, step: 1700, train_loss: 0.22113656997680664, test_loss: 10.23594856262207\n",
      "Epoch: 3, step: 1800, train_loss: 0.2203960418701172, test_loss: 10.248373985290527\n",
      "Epoch: 3, step: 1900, train_loss: 0.21491241455078125, test_loss: 10.257887840270996\n",
      "Epoch: 3, step: 2000, train_loss: 0.21957944333553314, test_loss: 10.284219741821289\n",
      "Epoch: 3, step: 2100, train_loss: 0.22523541748523712, test_loss: 10.304878234863281\n",
      "Epoch: 3, step: 2200, train_loss: 0.22208410501480103, test_loss: 10.295440673828125\n",
      "Epoch: 3, step: 2300, train_loss: 0.21481239795684814, test_loss: 10.295104026794434\n",
      "Epoch: 3, step: 2400, train_loss: 0.2291068136692047, test_loss: 10.284073829650879\n",
      "Epoch: 3, step: 2500, train_loss: 0.2217925637960434, test_loss: 10.285863876342773\n",
      "Epoch: 3, step: 2600, train_loss: 0.21580177545547485, test_loss: 10.300884246826172\n",
      "Epoch: 3, step: 2700, train_loss: 0.22237887978553772, test_loss: 10.308403015136719\n",
      "Epoch: 3, step: 2800, train_loss: 0.222890704870224, test_loss: 10.271653175354004\n",
      "Epoch: 3, step: 2900, train_loss: 0.21058040857315063, test_loss: 10.262933731079102\n",
      "Epoch: 3, step: 3000, train_loss: 0.21806600689888, test_loss: 10.226993560791016\n",
      "Epoch: 3, step: 3100, train_loss: 0.2198108732700348, test_loss: 10.245718002319336\n",
      "Epoch: 3, step: 3200, train_loss: 0.22933626174926758, test_loss: 10.262651443481445\n",
      "Epoch: 3, step: 3300, train_loss: 0.2130134552717209, test_loss: 10.339578628540039\n",
      "Epoch: 3, step: 3400, train_loss: 0.21357834339141846, test_loss: 10.312817573547363\n",
      "Epoch: 3, step: 3500, train_loss: 0.21697090566158295, test_loss: 10.268214225769043\n",
      "Epoch: 3, step: 3600, train_loss: 0.21124756336212158, test_loss: 10.269477844238281\n",
      "Epoch: 3, step: 3700, train_loss: 0.2158227413892746, test_loss: 10.233566284179688\n",
      "Epoch: 3, step: 3800, train_loss: 0.2070101946592331, test_loss: 10.296889305114746\n",
      "Epoch: 3, step: 3900, train_loss: 0.21606968343257904, test_loss: 10.324066162109375\n",
      "Epoch: 3, step: 4000, train_loss: 0.20009517669677734, test_loss: 10.313390731811523\n",
      "Epoch: 3, step: 4100, train_loss: 0.21632984280586243, test_loss: 10.278093338012695\n",
      "Epoch: 3, step: 4200, train_loss: 0.21841193735599518, test_loss: 10.224126815795898\n",
      "Epoch: 3, step: 4300, train_loss: 0.21570469439029694, test_loss: 10.205364227294922\n",
      "Epoch: 3, step: 4400, train_loss: 0.2152942717075348, test_loss: 10.199871063232422\n",
      "Epoch: 3, step: 4500, train_loss: 0.20639093220233917, test_loss: 10.243483543395996\n",
      "Epoch: 3, step: 4600, train_loss: 0.2082992047071457, test_loss: 10.251643180847168\n",
      "Epoch: 3, step: 4700, train_loss: 0.20275087654590607, test_loss: 10.270891189575195\n",
      "Epoch: 3, step: 4800, train_loss: 0.20425450801849365, test_loss: 10.304695129394531\n",
      "Epoch: 3, step: 4900, train_loss: 0.2063053846359253, test_loss: 10.2795991897583\n",
      "Epoch: 3, step: 5000, train_loss: 0.19766917824745178, test_loss: 10.272510528564453\n",
      "Epoch: 3, step: 5100, train_loss: 0.21100689470767975, test_loss: 10.210742950439453\n",
      "Epoch: 3, step: 5200, train_loss: 0.20648235082626343, test_loss: 10.287736892700195\n",
      "Epoch: 3, step: 5300, train_loss: 0.200081005692482, test_loss: 10.267809867858887\n",
      "Epoch: 3, step: 5400, train_loss: 0.2141362726688385, test_loss: 10.251731872558594\n",
      "Epoch: 3, step: 5500, train_loss: 0.21192821860313416, test_loss: 10.284089088439941\n",
      "Epoch: 3, step: 5600, train_loss: 0.20162373781204224, test_loss: 10.31364631652832\n",
      "Epoch: 3, step: 5700, train_loss: 0.2148306667804718, test_loss: 10.294585227966309\n",
      "Epoch: 3, step: 5800, train_loss: 0.20711253583431244, test_loss: 10.201301574707031\n",
      "Epoch: 3, step: 5900, train_loss: 0.207522913813591, test_loss: 10.279321670532227\n",
      "Epoch: 3, step: 6000, train_loss: 0.19711948931217194, test_loss: 10.278233528137207\n",
      "Epoch: 3, step: 6100, train_loss: 0.1899145543575287, test_loss: 10.329154014587402\n",
      "Epoch: 3, step: 6200, train_loss: 0.20560187101364136, test_loss: 10.32253646850586\n",
      "Epoch: 3, step: 6300, train_loss: 0.20256492495536804, test_loss: 10.216870307922363\n",
      "Epoch: 3, step: 6400, train_loss: 0.20586150884628296, test_loss: 10.236492156982422\n",
      "Epoch: 3, step: 6500, train_loss: 0.20594963431358337, test_loss: 10.260699272155762\n",
      "Epoch: 3, step: 6600, train_loss: 0.20617146790027618, test_loss: 10.270255088806152\n",
      "Epoch: 3, step: 6700, train_loss: 0.20603002607822418, test_loss: 10.299285888671875\n",
      "Epoch: 3, step: 6800, train_loss: 0.20535539090633392, test_loss: 10.266505241394043\n",
      "Epoch: 3, step: 6900, train_loss: 0.20780691504478455, test_loss: 10.294360160827637\n",
      "Epoch: 3, step: 7000, train_loss: 0.19744634628295898, test_loss: 10.34549331665039\n",
      "Epoch: 3, step: 7100, train_loss: 0.21331053972244263, test_loss: 10.304643630981445\n",
      "Epoch: 3, step: 7200, train_loss: 0.20473840832710266, test_loss: 10.235785484313965\n",
      "Epoch: 3, step: 7300, train_loss: 0.20501238107681274, test_loss: 10.232017517089844\n",
      "Epoch: 3, step: 7400, train_loss: 0.20062601566314697, test_loss: 10.268416404724121\n",
      "Epoch: 3, step: 7500, train_loss: 0.19325853884220123, test_loss: 10.310659408569336\n",
      "Epoch: 3, step: 7600, train_loss: 0.20269951224327087, test_loss: 10.29254150390625\n",
      "Epoch: 3, step: 7700, train_loss: 0.19620729982852936, test_loss: 10.313996315002441\n",
      "Epoch: 3, step: 7800, train_loss: 0.19706392288208008, test_loss: 10.271876335144043\n",
      "Epoch: 3, step: 7900, train_loss: 0.20416557788848877, test_loss: 10.289606094360352\n",
      "Epoch: 3, step: 8000, train_loss: 0.19715246558189392, test_loss: 10.31029987335205\n",
      "Epoch: 3, step: 8100, train_loss: 0.2034331113100052, test_loss: 10.340152740478516\n",
      "Epoch: 3, step: 8200, train_loss: 0.19597128033638, test_loss: 10.270305633544922\n",
      "Epoch: 3, step: 8300, train_loss: 0.20144839584827423, test_loss: 10.343518257141113\n",
      "Epoch: 3, step: 8400, train_loss: 0.1958426982164383, test_loss: 10.321637153625488\n",
      "Epoch: 3, step: 8500, train_loss: 0.19719558954238892, test_loss: 10.33090591430664\n",
      "Epoch: 3, step: 8600, train_loss: 0.2123495638370514, test_loss: 10.240076065063477\n",
      "Epoch: 3, step: 8700, train_loss: 0.19089290499687195, test_loss: 10.240521430969238\n",
      "Epoch: 3, step: 8800, train_loss: 0.18700557947158813, test_loss: 10.340520858764648\n",
      "Epoch: 3, step: 8900, train_loss: 0.18887831270694733, test_loss: 10.3145112991333\n",
      "Epoch: 3, step: 9000, train_loss: 0.19181109964847565, test_loss: 10.250537872314453\n",
      "Epoch: 3, step: 9096, train_loss: 0.1984841674566269, test_loss: 10.330926895141602\n",
      "Epoch: 4, step: 0, train_loss: 0.19996671378612518, test_loss: 10.334874153137207\n",
      "Epoch: 4, step: 100, train_loss: 0.19492778182029724, test_loss: 10.331295013427734\n",
      "Epoch: 4, step: 200, train_loss: 0.1894828975200653, test_loss: 10.287298202514648\n",
      "Epoch: 4, step: 300, train_loss: 0.19858019053936005, test_loss: 10.271581649780273\n",
      "Epoch: 4, step: 400, train_loss: 0.19550807774066925, test_loss: 10.319292068481445\n",
      "Epoch: 4, step: 500, train_loss: 0.19127261638641357, test_loss: 10.288331031799316\n",
      "Epoch: 4, step: 600, train_loss: 0.18547175824642181, test_loss: 10.34739875793457\n",
      "Epoch: 4, step: 700, train_loss: 0.18407383561134338, test_loss: 10.380146026611328\n",
      "Epoch: 4, step: 800, train_loss: 0.18749389052391052, test_loss: 10.321455001831055\n",
      "Epoch: 4, step: 900, train_loss: 0.1919621229171753, test_loss: 10.376608848571777\n",
      "Epoch: 4, step: 1000, train_loss: 0.20092472434043884, test_loss: 10.348297119140625\n",
      "Epoch: 4, step: 1100, train_loss: 0.19865207374095917, test_loss: 10.309416770935059\n",
      "Epoch: 4, step: 1200, train_loss: 0.1855800300836563, test_loss: 10.295075416564941\n",
      "Epoch: 4, step: 1300, train_loss: 0.19956669211387634, test_loss: 10.351418495178223\n",
      "Epoch: 4, step: 1400, train_loss: 0.18974025547504425, test_loss: 10.3375825881958\n",
      "Epoch: 4, step: 1500, train_loss: 0.18060900270938873, test_loss: 10.419367790222168\n",
      "Epoch: 4, step: 1600, train_loss: 0.18277303874492645, test_loss: 10.356269836425781\n",
      "Epoch: 4, step: 1700, train_loss: 0.18513715267181396, test_loss: 10.342945098876953\n",
      "Epoch: 4, step: 1800, train_loss: 0.19694076478481293, test_loss: 10.311022758483887\n",
      "Epoch: 4, step: 1900, train_loss: 0.19346995651721954, test_loss: 10.446022987365723\n",
      "Epoch: 4, step: 2000, train_loss: 0.19566479325294495, test_loss: 10.352094650268555\n",
      "Epoch: 4, step: 2100, train_loss: 0.18991799652576447, test_loss: 10.334766387939453\n",
      "Epoch: 4, step: 2200, train_loss: 0.1870780736207962, test_loss: 10.383818626403809\n",
      "Epoch: 4, step: 2300, train_loss: 0.19306163489818573, test_loss: 10.337355613708496\n",
      "Epoch: 4, step: 2400, train_loss: 0.18396008014678955, test_loss: 10.356999397277832\n",
      "Epoch: 4, step: 2500, train_loss: 0.18583768606185913, test_loss: 10.363842010498047\n",
      "Epoch: 4, step: 2600, train_loss: 0.1872350126504898, test_loss: 10.370421409606934\n",
      "Epoch: 4, step: 2700, train_loss: 0.1911781281232834, test_loss: 10.303544044494629\n",
      "Epoch: 4, step: 2800, train_loss: 0.19758348166942596, test_loss: 10.318913459777832\n",
      "Epoch: 4, step: 2900, train_loss: 0.20581340789794922, test_loss: 10.3562650680542\n",
      "Epoch: 4, step: 3000, train_loss: 0.18756118416786194, test_loss: 10.28585433959961\n",
      "Epoch: 4, step: 3100, train_loss: 0.1808042973279953, test_loss: 10.300357818603516\n",
      "Epoch: 4, step: 3200, train_loss: 0.18460582196712494, test_loss: 10.370523452758789\n",
      "Epoch: 4, step: 3300, train_loss: 0.18154554069042206, test_loss: 10.391684532165527\n",
      "Epoch: 4, step: 3400, train_loss: 0.18545496463775635, test_loss: 10.29738998413086\n",
      "Epoch: 4, step: 3500, train_loss: 0.18063707649707794, test_loss: 10.335121154785156\n",
      "Epoch: 4, step: 3600, train_loss: 0.1986968219280243, test_loss: 10.273853302001953\n",
      "Epoch: 4, step: 3700, train_loss: 0.18543148040771484, test_loss: 10.336514472961426\n",
      "Epoch: 4, step: 3800, train_loss: 0.1861504763364792, test_loss: 10.312725067138672\n",
      "Epoch: 4, step: 3900, train_loss: 0.17773813009262085, test_loss: 10.318007469177246\n",
      "Epoch: 4, step: 4000, train_loss: 0.1860451102256775, test_loss: 10.282866477966309\n",
      "Epoch: 4, step: 4100, train_loss: 0.18903972208499908, test_loss: 10.307260513305664\n",
      "Epoch: 4, step: 4200, train_loss: 0.18035903573036194, test_loss: 10.272316932678223\n",
      "Epoch: 4, step: 4300, train_loss: 0.18625976145267487, test_loss: 10.279890060424805\n",
      "Epoch: 4, step: 4400, train_loss: 0.18848024308681488, test_loss: 10.353950500488281\n",
      "Epoch: 4, step: 4500, train_loss: 0.18112502992153168, test_loss: 10.360384941101074\n",
      "Epoch: 4, step: 4600, train_loss: 0.1820600926876068, test_loss: 10.371662139892578\n",
      "Epoch: 4, step: 4700, train_loss: 0.17935432493686676, test_loss: 10.377477645874023\n",
      "Epoch: 4, step: 4800, train_loss: 0.19571809470653534, test_loss: 10.321977615356445\n",
      "Epoch: 4, step: 4900, train_loss: 0.18996022641658783, test_loss: 10.387899398803711\n",
      "Epoch: 4, step: 5000, train_loss: 0.17903751134872437, test_loss: 10.259063720703125\n",
      "Epoch: 4, step: 5100, train_loss: 0.17085418105125427, test_loss: 10.373737335205078\n",
      "Epoch: 4, step: 5200, train_loss: 0.17708252370357513, test_loss: 10.323690414428711\n",
      "Epoch: 4, step: 5300, train_loss: 0.18853573501110077, test_loss: 10.260002136230469\n",
      "Epoch: 4, step: 5400, train_loss: 0.1835474818944931, test_loss: 10.243908882141113\n",
      "Epoch: 4, step: 5500, train_loss: 0.18229210376739502, test_loss: 10.296402931213379\n",
      "Epoch: 4, step: 5600, train_loss: 0.18065984547138214, test_loss: 10.245614051818848\n",
      "Epoch: 4, step: 5700, train_loss: 0.17979268729686737, test_loss: 10.287718772888184\n",
      "Epoch: 4, step: 5800, train_loss: 0.17653626203536987, test_loss: 10.244708061218262\n",
      "Epoch: 4, step: 5900, train_loss: 0.18938767910003662, test_loss: 10.312172889709473\n",
      "Epoch: 4, step: 6000, train_loss: 0.17278049886226654, test_loss: 10.233078956604004\n",
      "Epoch: 4, step: 6100, train_loss: 0.18284308910369873, test_loss: 10.259498596191406\n",
      "Epoch: 4, step: 6200, train_loss: 0.17801325023174286, test_loss: 10.322248458862305\n",
      "Epoch: 4, step: 6300, train_loss: 0.17178192734718323, test_loss: 10.215643882751465\n",
      "Epoch: 4, step: 6400, train_loss: 0.18107032775878906, test_loss: 10.304847717285156\n",
      "Epoch: 4, step: 6500, train_loss: 0.1842917948961258, test_loss: 10.287956237792969\n",
      "Epoch: 4, step: 6600, train_loss: 0.17871345579624176, test_loss: 10.232980728149414\n",
      "Epoch: 4, step: 6700, train_loss: 0.17373420298099518, test_loss: 10.268197059631348\n",
      "Epoch: 4, step: 6800, train_loss: 0.1846899837255478, test_loss: 10.228775978088379\n",
      "Epoch: 4, step: 6900, train_loss: 0.17590871453285217, test_loss: 10.238293647766113\n",
      "Epoch: 4, step: 7000, train_loss: 0.1741737574338913, test_loss: 10.258136749267578\n",
      "Epoch: 4, step: 7100, train_loss: 0.1679515838623047, test_loss: 10.305865287780762\n",
      "Epoch: 4, step: 7200, train_loss: 0.17394685745239258, test_loss: 10.33945083618164\n",
      "Epoch: 4, step: 7300, train_loss: 0.17927178740501404, test_loss: 10.285375595092773\n",
      "Epoch: 4, step: 7400, train_loss: 0.17381815612316132, test_loss: 10.311121940612793\n",
      "Epoch: 4, step: 7500, train_loss: 0.17674203217029572, test_loss: 10.31292724609375\n",
      "Epoch: 4, step: 7600, train_loss: 0.1927371770143509, test_loss: 10.294713973999023\n",
      "Epoch: 4, step: 7700, train_loss: 0.17510013282299042, test_loss: 10.335127830505371\n",
      "Epoch: 4, step: 7800, train_loss: 0.17762108147144318, test_loss: 10.396832466125488\n",
      "Epoch: 4, step: 7900, train_loss: 0.1769348531961441, test_loss: 10.325967788696289\n",
      "Epoch: 4, step: 8000, train_loss: 0.1702532172203064, test_loss: 10.325708389282227\n",
      "Epoch: 4, step: 8100, train_loss: 0.17533710598945618, test_loss: 10.333064079284668\n",
      "Epoch: 4, step: 8200, train_loss: 0.1702849268913269, test_loss: 10.37252426147461\n",
      "Epoch: 4, step: 8300, train_loss: 0.17127451300621033, test_loss: 10.330760955810547\n",
      "Epoch: 4, step: 8400, train_loss: 0.17664188146591187, test_loss: 10.278553009033203\n",
      "Epoch: 4, step: 8500, train_loss: 0.1731971800327301, test_loss: 10.296579360961914\n",
      "Epoch: 4, step: 8600, train_loss: 0.17357559502124786, test_loss: 10.3056640625\n",
      "Epoch: 4, step: 8700, train_loss: 0.17729240655899048, test_loss: 10.275726318359375\n",
      "Epoch: 4, step: 8800, train_loss: 0.17989329993724823, test_loss: 10.28222942352295\n",
      "Epoch: 4, step: 8900, train_loss: 0.16766881942749023, test_loss: 10.326722145080566\n",
      "Epoch: 4, step: 9000, train_loss: 0.16498757898807526, test_loss: 10.266958236694336\n",
      "Epoch: 4, step: 9096, train_loss: 0.1725340634584427, test_loss: 10.256694793701172\n",
      "Epoch: 5, step: 0, train_loss: 0.17335624992847443, test_loss: 10.249842643737793\n",
      "Epoch: 5, step: 100, train_loss: 0.17103837430477142, test_loss: 10.256325721740723\n",
      "Epoch: 5, step: 200, train_loss: 0.16599193215370178, test_loss: 10.311908721923828\n",
      "Epoch: 5, step: 300, train_loss: 0.1784576028585434, test_loss: 10.313196182250977\n",
      "Epoch: 5, step: 400, train_loss: 0.1747206300497055, test_loss: 10.346508026123047\n",
      "Epoch: 5, step: 500, train_loss: 0.17414027452468872, test_loss: 10.357099533081055\n",
      "Epoch: 5, step: 600, train_loss: 0.18034690618515015, test_loss: 10.325143814086914\n",
      "Epoch: 5, step: 700, train_loss: 0.16838400065898895, test_loss: 10.351646423339844\n",
      "Epoch: 5, step: 800, train_loss: 0.16804543137550354, test_loss: 10.356690406799316\n",
      "Epoch: 5, step: 900, train_loss: 0.16485820710659027, test_loss: 10.325094223022461\n",
      "Epoch: 5, step: 1000, train_loss: 0.1686742752790451, test_loss: 10.268503189086914\n",
      "Epoch: 5, step: 1100, train_loss: 0.1754702776670456, test_loss: 10.337772369384766\n",
      "Epoch: 5, step: 1200, train_loss: 0.167344868183136, test_loss: 10.306598663330078\n",
      "Epoch: 5, step: 1300, train_loss: 0.17751339077949524, test_loss: 10.270157814025879\n",
      "Epoch: 5, step: 1400, train_loss: 0.17594413459300995, test_loss: 10.408668518066406\n",
      "Epoch: 5, step: 1500, train_loss: 0.16913574934005737, test_loss: 10.372471809387207\n",
      "Epoch: 5, step: 1600, train_loss: 0.16939803957939148, test_loss: 10.361444473266602\n",
      "Epoch: 5, step: 1700, train_loss: 0.17384938895702362, test_loss: 10.350266456604004\n",
      "Epoch: 5, step: 1800, train_loss: 0.16632243990898132, test_loss: 10.309179306030273\n",
      "Epoch: 5, step: 1900, train_loss: 0.17919296026229858, test_loss: 10.324145317077637\n",
      "Epoch: 5, step: 2000, train_loss: 0.17435428500175476, test_loss: 10.32658576965332\n",
      "Epoch: 5, step: 2100, train_loss: 0.1761908084154129, test_loss: 10.420188903808594\n",
      "Epoch: 5, step: 2200, train_loss: 0.17715653777122498, test_loss: 10.314414024353027\n",
      "Epoch: 5, step: 2300, train_loss: 0.15991689264774323, test_loss: 10.372532844543457\n",
      "Epoch: 5, step: 2400, train_loss: 0.17049121856689453, test_loss: 10.419868469238281\n",
      "Epoch: 5, step: 2500, train_loss: 0.16550178825855255, test_loss: 10.37514877319336\n",
      "Epoch: 5, step: 2600, train_loss: 0.1696649044752121, test_loss: 10.348631858825684\n",
      "Epoch: 5, step: 2700, train_loss: 0.1656276136636734, test_loss: 10.372143745422363\n",
      "Epoch: 5, step: 2800, train_loss: 0.16389980912208557, test_loss: 10.45594596862793\n",
      "Epoch: 5, step: 2900, train_loss: 0.17474637925624847, test_loss: 10.337355613708496\n",
      "Epoch: 5, step: 3000, train_loss: 0.17302793264389038, test_loss: 10.282169342041016\n",
      "Epoch: 5, step: 3100, train_loss: 0.166387677192688, test_loss: 10.357312202453613\n",
      "Epoch: 5, step: 3200, train_loss: 0.1589202582836151, test_loss: 10.415495872497559\n",
      "Epoch: 5, step: 3300, train_loss: 0.16321712732315063, test_loss: 10.445914268493652\n",
      "Epoch: 5, step: 3400, train_loss: 0.17346668243408203, test_loss: 10.414813041687012\n",
      "Epoch: 5, step: 3500, train_loss: 0.17129065096378326, test_loss: 10.284014701843262\n",
      "Epoch: 5, step: 3600, train_loss: 0.16890694200992584, test_loss: 10.340648651123047\n",
      "Epoch: 5, step: 3700, train_loss: 0.16805289685726166, test_loss: 10.288779258728027\n",
      "Epoch: 5, step: 3800, train_loss: 0.16753263771533966, test_loss: 10.358399391174316\n",
      "Epoch: 5, step: 3900, train_loss: 0.16763156652450562, test_loss: 10.404862403869629\n",
      "Epoch: 5, step: 4000, train_loss: 0.16844692826271057, test_loss: 10.359029769897461\n",
      "Epoch: 5, step: 4100, train_loss: 0.16966110467910767, test_loss: 10.318938255310059\n",
      "Epoch: 5, step: 4200, train_loss: 0.15993830561637878, test_loss: 10.298652648925781\n",
      "Epoch: 5, step: 4300, train_loss: 0.16151276230812073, test_loss: 10.367116928100586\n",
      "Epoch: 5, step: 4400, train_loss: 0.16121093928813934, test_loss: 10.337075233459473\n",
      "Epoch: 5, step: 4500, train_loss: 0.1592409461736679, test_loss: 10.397337913513184\n",
      "Epoch: 5, step: 4600, train_loss: 0.15818706154823303, test_loss: 10.356623649597168\n",
      "Epoch: 5, step: 4700, train_loss: 0.15840311348438263, test_loss: 10.379219055175781\n",
      "Epoch: 5, step: 4800, train_loss: 0.17063425481319427, test_loss: 10.347870826721191\n",
      "Epoch: 5, step: 4900, train_loss: 0.1596718430519104, test_loss: 10.367591857910156\n",
      "Epoch: 5, step: 5000, train_loss: 0.16601957380771637, test_loss: 10.385100364685059\n",
      "Epoch: 5, step: 5100, train_loss: 0.16697806119918823, test_loss: 10.402429580688477\n",
      "Epoch: 5, step: 5200, train_loss: 0.15545648336410522, test_loss: 10.331491470336914\n",
      "Epoch: 5, step: 5300, train_loss: 0.16027311980724335, test_loss: 10.323525428771973\n",
      "Epoch: 5, step: 5400, train_loss: 0.16609369218349457, test_loss: 10.317758560180664\n",
      "Epoch: 5, step: 5500, train_loss: 0.16505084931850433, test_loss: 10.317541122436523\n",
      "Epoch: 5, step: 5600, train_loss: 0.1578783243894577, test_loss: 10.320786476135254\n",
      "Epoch: 5, step: 5700, train_loss: 0.15731145441532135, test_loss: 10.4100980758667\n",
      "Epoch: 5, step: 5800, train_loss: 0.16106387972831726, test_loss: 10.426831245422363\n",
      "Epoch: 5, step: 5900, train_loss: 0.17056028544902802, test_loss: 10.369095802307129\n",
      "Epoch: 5, step: 6000, train_loss: 0.16948740184307098, test_loss: 10.331376075744629\n",
      "Epoch: 5, step: 6100, train_loss: 0.1747978925704956, test_loss: 10.378396034240723\n",
      "Epoch: 5, step: 6200, train_loss: 0.16721153259277344, test_loss: 10.302376747131348\n",
      "Epoch: 5, step: 6300, train_loss: 0.1635451465845108, test_loss: 10.348588943481445\n",
      "Epoch: 5, step: 6400, train_loss: 0.15295252203941345, test_loss: 10.384145736694336\n",
      "Epoch: 5, step: 6500, train_loss: 0.14771783351898193, test_loss: 10.433279037475586\n",
      "Epoch: 5, step: 6600, train_loss: 0.16409164667129517, test_loss: 10.410425186157227\n",
      "Epoch: 5, step: 6700, train_loss: 0.17528872191905975, test_loss: 10.321812629699707\n",
      "Epoch: 5, step: 6800, train_loss: 0.16312211751937866, test_loss: 10.413089752197266\n",
      "Epoch: 5, step: 6900, train_loss: 0.16965198516845703, test_loss: 10.355439186096191\n",
      "Epoch: 5, step: 7000, train_loss: 0.1566256880760193, test_loss: 10.297673225402832\n",
      "Epoch: 5, step: 7100, train_loss: 0.1598941534757614, test_loss: 10.361985206604004\n",
      "Epoch: 5, step: 7200, train_loss: 0.16318073868751526, test_loss: 10.297441482543945\n",
      "Epoch: 5, step: 7300, train_loss: 0.16352631151676178, test_loss: 10.303577423095703\n",
      "Epoch: 5, step: 7400, train_loss: 0.1581481695175171, test_loss: 10.33500862121582\n",
      "Epoch: 5, step: 7500, train_loss: 0.1576080620288849, test_loss: 10.398533821105957\n",
      "Epoch: 5, step: 7600, train_loss: 0.16495832800865173, test_loss: 10.337798118591309\n",
      "Epoch: 5, step: 7700, train_loss: 0.15129517018795013, test_loss: 10.336395263671875\n",
      "Epoch: 5, step: 7800, train_loss: 0.15619231760501862, test_loss: 10.407840728759766\n",
      "Epoch: 5, step: 7900, train_loss: 0.15480925142765045, test_loss: 10.434366226196289\n",
      "Epoch: 5, step: 8000, train_loss: 0.16284534335136414, test_loss: 10.443050384521484\n",
      "Epoch: 5, step: 8100, train_loss: 0.15629181265830994, test_loss: 10.351287841796875\n",
      "Epoch: 5, step: 8200, train_loss: 0.15464290976524353, test_loss: 10.474251747131348\n",
      "Epoch: 5, step: 8300, train_loss: 0.1518741399049759, test_loss: 10.421710014343262\n",
      "Epoch: 5, step: 8400, train_loss: 0.1461675763130188, test_loss: 10.415038108825684\n",
      "Epoch: 5, step: 8500, train_loss: 0.16479578614234924, test_loss: 10.350186347961426\n",
      "Epoch: 5, step: 8600, train_loss: 0.15542560815811157, test_loss: 10.385333061218262\n",
      "Epoch: 5, step: 8700, train_loss: 0.16369743645191193, test_loss: 10.416892051696777\n",
      "Epoch: 5, step: 8800, train_loss: 0.15431320667266846, test_loss: 10.439391136169434\n",
      "Epoch: 5, step: 8900, train_loss: 0.16339418292045593, test_loss: 10.44869613647461\n",
      "Epoch: 5, step: 9000, train_loss: 0.15890075266361237, test_loss: 10.349410057067871\n",
      "Epoch: 5, step: 9096, train_loss: 0.15957625210285187, test_loss: 10.405240058898926\n",
      "Epoch: 6, step: 0, train_loss: 0.16146652400493622, test_loss: 10.402865409851074\n",
      "Epoch: 6, step: 100, train_loss: 0.1575908064842224, test_loss: 10.387106895446777\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     output, loss \u001b[38;5;241m=\u001b[39m model(x, y)\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m save_checkpoint(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../output/checkpoints/model2/checkpoint_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     24\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tanis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tanis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tanis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    843\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch_index, (x, y) in enumerate(train_loader):\n",
    "        if batch_index % evaluation_iterations == 0 or batch_index == len(train_loader) - 1:\n",
    "            losses = get_model_loss(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                test_loader=test_loader,\n",
    "                evaluation_iterations=evaluation_iterations\n",
    "            )\n",
    "\n",
    "            print(f'Epoch: {epoch}, step: {batch_index}, train_loss: {losses[\"train\"]}, test_loss: {losses[\"test\"]}')\n",
    "        \n",
    "        output, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        loss=loss.item(),\n",
    "        filename=f'../output/checkpoints/model2/checkpoint_{epoch}.pth'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer.encode(\"Sup fam\")\n",
    "input_tokens = torch.tensor(input_tokens, dtype=torch.long).reshape(1,4).to(device)\n",
    "print(input_tokens.shape)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(tokens=input_tokens, max_token_limit=500)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "549d7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../output/models/model2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
