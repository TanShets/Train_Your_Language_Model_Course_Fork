{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1236a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adda70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe.gpt4 import GPT4Tokenizer\n",
    "from minbpe.basic import BasicTokenizer\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(model_file='../output/tokenizer/temp_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "102e1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "block_size = 512\n",
    "embedding_dimension = 128\n",
    "no_of_attention_heads = 8\n",
    "key_query_reduced_dimensionality = 8\n",
    "no_of_layers = 4\n",
    "vocab_size = len(tokenizer.vocab)+len(tokenizer.special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d0e6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1+cu126'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d331955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from typing import Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "782cd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, key_query_reduced_dimensionality, embedding_dimension):\n",
    "        super().__init__()\n",
    "        self.key_query_reduced_dimensionality = key_query_reduced_dimensionality\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.query = nn.Linear(self.embedding_dimension, self.key_query_reduced_dimensionality, bias = False)\n",
    "        self.key = nn.Linear(self.embedding_dimension, self.key_query_reduced_dimensionality, bias = False)\n",
    "        self.value = nn.Linear(self.embedding_dimension, self.key_query_reduced_dimensionality, bias = False)\n",
    "    \n",
    "    def forward(self, embeddedVector: torch.Tensor) -> torch.Tensor:\n",
    "        _, T, _ = embeddedVector.shape\n",
    "        Q = self.query(embeddedVector)\n",
    "        K = self.key(embeddedVector)\n",
    "        V = self.value(embeddedVector)\n",
    "\n",
    "        KT = torch.transpose(K, 0, 1)\n",
    "        R = torch.matmul(KT, Q) / (self.embedding_dimension**0.5)\n",
    "        R = R.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        R = F.softmax(R, dim = -1)\n",
    "        change_in_embedded_vector = R @ V\n",
    "\n",
    "        return change_in_embedded_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b8c3a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 128])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = nn.Linear(128, 8, bias = False)\n",
    "key = nn.Linear(128, 8, bias = False)\n",
    "value = nn.Linear(128, 128, bias = False)\n",
    "embeddedVector = torch.randn(50, 128)\n",
    "_, T = embeddedVector.shape\n",
    "Q = query(embeddedVector)\n",
    "K = key(embeddedVector)\n",
    "V = value(embeddedVector)\n",
    "\n",
    "QT = torch.transpose(Q, 0, 1)\n",
    "R = torch.matmul(K, QT) / (embedding_dimension**0.5)\n",
    "R = R.masked_fill(torch.transpose(torch.tril(R), 0, 1)[:T, :T] == 0, float('-inf'))\n",
    "R = F.softmax(R, dim = -1)\n",
    "(R @ V).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a0d2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, no_of_attention_heads, key_query_reduced_dimensionality, embedding_dimension):\n",
    "        super().__init__()\n",
    "        self.no_of_attention_heads = no_of_attention_heads\n",
    "        self.key_query_reduced_dimensionality = key_query_reduced_dimensionality\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.heads = nn.ModuleList([AttentionHead(key_query_reduced_dimensionality, embeddding_dimension) for _ in range(no_of_attention_heads)])\n",
    "        # Originally We could have considered each Value Weighr Wv to be (embedded_dimension, embedded_dimension) but that's too massive so we can alt do (embedded_dimension, reduced_dim) x (reduced_dim, embedded_dimension)\n",
    "        # That's what we do in actual transformers, so that's (Output Weight) x (Value Weight new), Value Weight goes to each head and Output Weight is just joined together for all heads\n",
    "        self.output_weight = nn.Linear(self.no_of_attention_heads * self.key_query_reduced_dimensionality, self.embedding_dimension)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "    \n",
    "    def forward(self, embeddedVector: torch.Tensor) -> torch.Tensor:\n",
    "        output = torch.cat([attentionHead(embeddedVector) for attentionHead in self.heads], dim = -1)\n",
    "        output = self.dropout(self.output_weight(output))\n",
    "        return output\n",
    "\n",
    "#Multilayer Perceptron Layer\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, embedding_dimension, expansion_factor = 4):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        #Expanding Layer which by default will be set to 4 times the size of the embedded dimension\n",
    "        self.neural_net = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dimension, self.expansion_factor * self.embedding_dimension),\n",
    "            nn.ReLu(),\n",
    "            nn.Linear(self.expansion_factor * self.embedding_dimension, self.embedding_dimension),\n",
    "            nn.Dropout(p = 0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddedVector: torch.Tensor) -> torch.Tensor:\n",
    "        return self.neural_net(embeddedVector)        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bacf654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dimension, key_query_reduced_dimensionality, no_of_attention_heads, expansion_factor = 4):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        \n",
    "        self.attention_layer = AttentionLayer(no_of_attention_heads, key_query_reduced_dimensionality, self.embedding_dimension),\n",
    "        self.normal_1 = nn.LayerNorm(self.embedding_dimension)\n",
    "        self.mlp_layer = MLPLayer(self.embedding_dimension, expansion_factor)\n",
    "        self.normal_2 = nn.LayerNorm(self.embedding_dimension)\n",
    "    \n",
    "    def forward(self, embeddedVector: torch.Tensor) -> torch.Tensor:\n",
    "        embeddedVector += self.attention_layer(self.normal_1(embeddedVector))\n",
    "        embeddedVector += self.mlp_layer(self.normal_2(embeddedVector))\n",
    "        return embeddedVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16115da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTTransformer(nn.Module):\n",
    "    def __init__(self, context_size, no_of_blocks, embedding_dimension, key_query_reduced_dimensionality, no_of_attention_heads, expansion_factor = 4):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.no_of_blocks = no_of_blocks\n",
    "\n",
    "        #Embedding first\n",
    "        self.token_embedder = nn.Embedding(vocab_size, embedding_dimension)\n",
    "        self.position_embedder = nn.Embedding(context_size, embedding_dimension)\n",
    "        #All transformations\n",
    "        self.transformer_blocks = nn.Sequential([TransformerBlock(embedding_dimension, key_query_reduced_dimensionality, no_of_attention_heads, expansion_factor) for _ in range(no_of_blocks)])\n",
    "\n",
    "        self.normal = nn.LayerNorm(embedding_dimension)\n",
    "        self.final_layer = nn.Linear(embedding_dimension, vocab_size)\n",
    "    \n",
    "    def forward(self, tokens: torch.Tensor, ideal_value: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        _, T, _ = tokens.shape\n",
    "        token_embedding = self.token_embedder(tokens)\n",
    "        position_embedding = self.position_embedder(torch.arange(T, device = device))\n",
    "\n",
    "        embeddedVector = token_embedding + position_embedding\n",
    "\n",
    "        output = self.transformer_blocks(embeddedVector)\n",
    "        output = self.normal(output)\n",
    "        output = self.final_layer(output)\n",
    "\n",
    "        if ideal_value is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = output.shape\n",
    "            output = output.view(B * T, C)\n",
    "            ideal_value = ideal_value.view(B * T, C)\n",
    "\n",
    "            loss = F.cross_entropy(output, ideal_value)\n",
    "        \n",
    "        return output, loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
